{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Repeatable Evaluations with an Eval Harness  \n",
    "\n",
    "After experimenting informally, we need a **systematic way to measure performance**. This part introduces an **evaluation harness**‚Äîa structured script for running batch tests across multiple inputs.  \n",
    "\n",
    "We‚Äôll cover:  \n",
    "- **Defining evaluation criteria** based on observed patterns.  \n",
    "- **Running batch tests** using a dataset of team names.  \n",
    "- **Expanding evaluation** to real-world data, like news articles.  \n",
    "\n",
    "By the end, you‚Äôll have a **repeatable evaluation process**, ensuring that our AI system‚Äôs performance is measurable and consistent across different inputs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "model = 'gemma2:2b'\n",
    "\n",
    "def single_turn(prompt):\n",
    "    response: ChatResponse = chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "      },\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "prompt = \"Say hello to the class\"\n",
    "single_turn(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your First Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the list of AFL and NFL team names.\n",
    "Check if the model gets things right.\n",
    "This means you'll need to create a scoring and aggregation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "afl_clubs = [\n",
    "    \"Adelaide Crows\",\n",
    "    \"Brisbane Lions\",\n",
    "    \"Carlton Blues\",\n",
    "    \"Collingwood Magpies\",\n",
    "    \"Essendon Bombers\",\n",
    "    \"Fremantle Dockers\",\n",
    "    \"Geelong Cats\",\n",
    "    \"Gold Coast Suns\",\n",
    "    \"Greater Western Sydney (GWS) Giants\",\n",
    "    \"Hawthorn Hawks\",\n",
    "    \"Melbourne Demons\",\n",
    "    \"North Melbourne Kangaroos\",\n",
    "    \"Port Adelaide Power\",\n",
    "    \"Richmond Tigers\",\n",
    "    \"St Kilda Saints\",\n",
    "    \"Sydney Swans\",\n",
    "    \"West Coast Eagles\",\n",
    "    \"Western Bulldogs\"\n",
    "]\n",
    "\n",
    "nfl_teams = [\n",
    "    \"Arizona Cardinals\",\n",
    "    \"Atlanta Falcons\",\n",
    "    \"Baltimore Ravens\",\n",
    "    \"Buffalo Bills\",\n",
    "    \"Carolina Panthers\",\n",
    "    \"Chicago Bears\",\n",
    "    \"Cincinnati Bengals\",\n",
    "    \"Cleveland Browns\",\n",
    "    \"Dallas Cowboys\",\n",
    "    \"Denver Broncos\",\n",
    "    \"Detroit Lions\",\n",
    "    \"Green Bay Packers\",\n",
    "    \"Houston Texans\",\n",
    "    \"Indianapolis Colts\",\n",
    "    \"Jacksonville Jaguars\",\n",
    "    \"Kansas City Chiefs\",\n",
    "    \"Las Vegas Raiders\",\n",
    "    \"Los Angeles Chargers\",\n",
    "    \"Los Angeles Rams\",\n",
    "    \"Miami Dolphins\",\n",
    "    \"Minnesota Vikings\",\n",
    "    \"New England Patriots\",\n",
    "    \"New Orleans Saints\",\n",
    "    \"New York Giants\",\n",
    "    \"New York Jets\",\n",
    "    \"Philadelphia Eagles\",\n",
    "    \"Pittsburgh Steelers\",\n",
    "    \"San Francisco 49ers\",\n",
    "    \"Seattle Seahawks\",\n",
    "    \"Tampa Bay Buccaneers\",\n",
    "    \"Tennessee Titans\",\n",
    "    \"Washington Commanders\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "eval_map = {\"australian\": afl_clubs, \"american\": nfl_teams}\n",
    "\n",
    "# Remove key let students code themselves\n",
    "score = []\n",
    "for nationality, teams in eval_map.items():\n",
    "    for team in teams:\n",
    "        prompt = f\"Output if this is an australian or american team, only print australian or american no other output: {team}\"\n",
    "        response = single_turn(prompt).strip()\n",
    "        score.append(response.lower() == nationality)\n",
    "        print(f\"{team}: {response}\")\n",
    "\n",
    "print(np.array(score).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Notebook to Script: Making Evaluation Repeatable\n",
    "\n",
    "Now that we‚Äôve run our evaluation in the notebook, we want to make this process **repeatable and scriptable**.  \n",
    "\n",
    "Executing the following cell will **generate `traces.csv`**, storing the model's predictions alongside the ground truth.  \n",
    "\n",
    "Once the file is created, run the evaluation harness script to **compute accuracy programmatically**:  \n",
    "\n",
    "```\n",
    "python eval_harness.py data/traces.csv --output data/scored_results.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "eval_map = {\"australian\": afl_clubs, \"american\": nfl_teams}\n",
    "\n",
    "# Open CSV file for writing\n",
    "csv_filename = \"data/traces.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"team_name\", \"ground_truth\", \"model_output\"])\n",
    "\n",
    "    for nationality, teams in eval_map.items():\n",
    "        for team in teams:\n",
    "            prompt = f\"Output if this is an australian or american team, only print australian or american no other output {team}\"\n",
    "            response = single_turn(prompt).strip().lower()\n",
    "            \n",
    "            # Save to CSV\n",
    "            #writer.writerow([team, nationality, response])\n",
    "            \n",
    "            print(f\"{team}: {response}\")\n",
    "\n",
    "print(f\"üìÅ Saved results to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making things harder. Taking just the team name\n",
    "What happens if we don't provide the full context. What happens to our score then? Is this expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "afl_names = ['Crows',\n",
    " 'Lions',\n",
    " 'Blues',\n",
    " 'Magpies',\n",
    " 'Bombers',\n",
    " 'Dockers',\n",
    " 'Cats',\n",
    " 'Suns',\n",
    " 'Giants',\n",
    " 'Hawks',\n",
    " 'Demons',\n",
    " 'Kangaroos',\n",
    " 'Power',\n",
    " 'Tigers',\n",
    " 'Saints',\n",
    " 'Swans',\n",
    " 'Eagles',\n",
    " 'Bulldogs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_teams = ['Cardinals',\n",
    " 'Falcons',\n",
    " 'Ravens',\n",
    " 'Bills',\n",
    " 'Panthers',\n",
    " 'Bears',\n",
    " 'Bengals',\n",
    " 'Browns',\n",
    " 'Cowboys',\n",
    " 'Broncos',\n",
    " 'Lions',\n",
    " 'Packers',\n",
    " 'Texans',\n",
    " 'Colts',\n",
    " 'Jaguars',\n",
    " 'Chiefs',\n",
    " 'Raiders',\n",
    " 'Chargers',\n",
    " 'Rams',\n",
    " 'Dolphins',\n",
    " 'Vikings',\n",
    " 'Patriots',\n",
    " 'Saints',\n",
    " 'Giants',\n",
    " 'Jets',\n",
    " 'Eagles',\n",
    " 'Steelers',\n",
    " '49ers',\n",
    " 'Seahawks',\n",
    " 'Buccaneers',\n",
    " 'Titans',\n",
    " 'Commanders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_map = {\"american\": nfl_teams, \"australian\": afl_names}\n",
    "\n",
    "core = []\n",
    "\n",
    "for nationality, teams in eval_map.items():\n",
    "    for team in teams:\n",
    "        team = team.rsplit(maxsplit =1)[-1]\n",
    "        prompt = \"Output if this is an australian or american team, only print australian or american no other output: \" + f\"{team}\"\n",
    "        response = single_turn(prompt).strip()\n",
    "        score.append(response.lower() == nationality)\n",
    "        print(f\"{team}: {response}\")\n",
    "print(np.array(score).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Articles\n",
    "Let's now try this on full articles. We've provided two samples for you but you're going to need todo the bulk of the work here. That is\n",
    "\n",
    "1. Construct a larger eval set\n",
    "2. Store the expected result in a way that you can check the model response\n",
    "3. Run each article and check the result\n",
    "4. Aggregate the results and create a final metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_sources/article_2.txt\n",
      "Australian\n",
      "American\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "txt_files = glob.glob(\"text_sources/*.txt\")\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, 'r') as f:\n",
    "        article_text = f.read()\n",
    "        full_prompt = f\"print if this article is about an australian or american team, no other output: {article_text}\"\n",
    "\n",
    "        response = single_turn(full_prompt).strip()\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Recap: What We Learned  \n",
    "\n",
    "### Setting up repeatable eval infra\n",
    "* Evaling by vibes is great start but\n",
    "  * It's manual to rerun\n",
    "  * Hard to compare as projects scale\n",
    "  * Ambiguous to track as things change\n",
    "\n",
    "### Building your own evalsets\n",
    "* Evalsets are as critical as model and infra choice\n",
    "* No one knows your use case better than you\n",
    "* Building your own evalset will encourage deep thinking about your challenge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
