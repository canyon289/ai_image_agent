{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Building the MVP AI System  \n",
    "\n",
    "**NOTE**: This all needs to be updated\n",
    "\n",
    "In this first part, weâ€™ll build a **function calling framework** using a local **LLM-powered approach**. Instead of training a model from scratch, weâ€™ll leverage **Gemma 3B** and instead of using a framework we'll code eveything **from scratch**\n",
    "\n",
    "Agents are **more than just a model**. We'll also integrate:  \n",
    "- **Gradio** to build an interactive front end.  \n",
    "- **SQLite** to store data and results.  \n",
    "- **Datasette** for observability, allowing us to inspect predictions and iterate effectively.  \n",
    "\n",
    "By the end of this section, youâ€™ll have a **working MVP AI system**â€”a functional app with a front end, database, and structured observability to track and refine performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Gemma 2B and Ollama?\n",
    "\n",
    "### ğŸ”¹ Gemma 2B: A Small but Mighty LLM  \n",
    "\n",
    "![Alt text](img/gemma2-2B.png)\n",
    "\n",
    "[Gemma](https://ai.google.dev/gemma) is a family of open-weight models from Google DeepMind, designed for efficiency and strong reasoning capabilities.  \n",
    "- **2B parameters**: Small enough to run locally but powerful enough for real tasks.  \n",
    "- **Supports in-context learning**: Like other transformer models, Gemma can classify and generate outputs based on provided examples without additional training.  \n",
    "- **Works well for zero-shot classification**: Like other modern LLMs, Gemma can classify inputs based on prompts without fine-tuning.  \n",
    "- **Fast and cost-effective**: Unlike larger models, Gemma 2B runs efficiently on consumer hardware.  \n",
    "\n",
    "### ğŸš€ Ollama: A Game Changer for Local LLMs  \n",
    "\n",
    "![Alt text](img/ollama.svg)\n",
    "\n",
    "[Ollama](https://ollama.com/) makes running **LLMs locally** seamless, without complex setup.  \n",
    "- **Pre-configured model execution**: No need to manually set up dependencies.  \n",
    "- **Efficient GPU/CPU inference**: Optimized for running on local machines.  \n",
    "- **Fast iteration loop**: Load a model once, then run queries without excessive overhead.  \n",
    "\n",
    "By combining **Gemma 2B** with **Ollama**, we get a **lightweight, fast, and cost-free AI system** that can perform real-world classification tasks directly on our machines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello everyone! ğŸ‘‹  ğŸ˜Š \\n\\nIs there anything I can help you with today? \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "model = 'gemma2:2b'\n",
    "\n",
    "def single_turn(prompt):\n",
    "    response: ChatResponse = chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "      },\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "prompt = \"Say hello to the class\"\n",
    "single_turn(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our zero-shot classification task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Creating Our Gradio App  \n",
    "\n",
    "![Alt text](img/gradio.png)\n",
    "\n",
    "Before we dive into the code, let's talk about **Gradio**â€”one of the easiest ways to spin up interactive front-ends for AI applications.  \n",
    "\n",
    "ğŸš€ **Why Gradio?**  \n",
    "- **Super fast MVP development**: Build an interactive AI demo in just a few lines of code.  \n",
    "- **No frontend experience required**: Just define a Python function, and Gradio handles the UI.  \n",
    "- **Part of the ğŸ¤— Hugging Face ecosystem**: Seamlessly integrates with **models, Spaces, and APIs**.  \n",
    "- **Great for rapid prototyping**: Test AI models with real users before scaling up.  \n",
    "\n",
    "We'll use **Gradio** to build an interactive app that lets users test **Gemma 2B** for zero-shot classificationâ€”without needing a separate web framework. Let's get started! ğŸš€  \n",
    "\n",
    "\n",
    "For instruction purposes, we've included the code below, but we'll be running our apps from the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gradio as gr\n",
    "import ollama\n",
    "\n",
    "model = 'gemma:2b'\n",
    "\n",
    "def chat_with_model(prompt):\n",
    "    response = ollama.chat(model=model, messages=[{'role': 'user', 'content': prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type your message here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Chat with Gemma\",\n",
    "    description=\"Enter a message and get a response from the Gemma 2B model.\",\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ What's Happening in This Code?  \n",
    "\n",
    "- ğŸ”„ **Imports Gradio & Ollama** â€“ We bring in the tools we need to build the UI and interact with the model.  \n",
    "- ğŸ§  **Defines the model** â€“ We're using **Gemma 2B** (`gemma:2b`) to power the chatbot.  \n",
    "- ğŸ’¬ **Creates a function (`chat_with_model`)** â€“ Sends user input to the model via **Ollama** and returns a response.  \n",
    "- ğŸ¨ **Builds the Gradio UI (`iface`)** â€“  \n",
    "  - **ğŸ“© Input**: A text box for user messages.  \n",
    "  - **ğŸ–¥ï¸ Output**: The model's response.  \n",
    "  - **ğŸ­ Title & Description**: A simple interface for chatting with Gemma.  \n",
    "- ğŸš€ **Launches the app!** â€“ Runs the interactive chatbot in your browser.  \n",
    "\n",
    "Now, letâ€™s fire it up and start chatting! ğŸ”¥  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding observability with SQLite and Datasette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Why Tracing & Observability Matter\n",
    "\n",
    "Building an AI system isnâ€™t just about **getting a response**â€”itâ€™s about **understanding and improving how your model behaves over time**.  \n",
    "- **ğŸ‘€ Observability** helps us track inputs, outputs, and model decisions, making debugging and iteration easier.  \n",
    "- **ğŸ› Tracing conversations** lets us spot patterns, catch failure cases, and fine-tune our system for better performance.  \n",
    "- **ğŸ“ˆ Data-Driven Decisions**: Instead of guessing if the model is working well, we can use **real logged interactions** to refine prompts, improve accuracy, and compare models.  \n",
    "\n",
    "## ğŸ—„ï¸ Why SQLite? A No-Brainer for MVPs  \n",
    "\n",
    "![Alt text](img/sqlite.png)\n",
    "\n",
    "For **early-stage apps**, **SQLite** is an **ideal** choice for logging and observability:  \n",
    "- **ğŸ› ï¸ No Setup Hassle** â€“ Itâ€™s a self-contained, file-based database. No server required.  \n",
    "- **âš¡ Fast & Lightweight** â€“ Handles reads and writes efficiently without extra overhead.  \n",
    "- **ğŸ“¦ Portable & Easy to Share** â€“ Just a single file (`.db`) that works across different environments.  \n",
    "- **ğŸ”— Overwhelmingly Popular** â€“ Used in everything from **mobile apps (iOS, Android)** to **browsers (Chrome, Firefox)** and even **airplane black boxes**!  \n",
    "\n",
    "### ğŸš€ Future Scaling  \n",
    "Right now, **SQLite is perfect** for logging and inspecting model interactions. Later, if we move to **multi-user or production-scale apps**, we can switch to **PostgreSQL, MySQL, or cloud-based solutions**â€”but for now, SQLite keeps things simple and effective.  \n",
    "\n",
    "---\n",
    "\n",
    "Next, weâ€™ll **log our prompts and responses** so we can start analyzing how our system is performing! ğŸ”\n",
    "\n",
    " As above, we've included the code below, although we'll be running our apps from the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gradio as gr\n",
    "import ollama\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# SQLite Database Setup\n",
    "DB_PATH = \"chat_log.db\"\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Create a simple SQLite table if it doesn't exist.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS chat_history (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            prompt TEXT,\n",
    "            response TEXT,\n",
    "            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "setup_database()  # Ensure the DB is set up before running the app\n",
    "\n",
    "def chat_with_model(prompt):\n",
    "    \"\"\"Send user input to Ollama, get response, and log to SQLite.\"\"\"\n",
    "    response = ollama.chat(model=\"gemma:2b\", messages=[{\"role\": \"user\", \"content\": prompt}])[\"message\"][\"content\"]\n",
    "    \n",
    "    # Log the interaction to SQLite\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"INSERT INTO chat_history (prompt, response) VALUES (?, ?)\", (prompt, response))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Gradio UI\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type your message here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Chat with Gemma\",\n",
    "    description=\"Enter a message and get a response from the Gemma 2B model. Your chats are logged in SQLite.\",\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ What's Happening in This Code?  \n",
    "\n",
    "- ğŸ“¦ **Imports required libraries** â€“  \n",
    "  - `gradio` for the UI  \n",
    "  - `ollama` for running **Gemma 2B**  \n",
    "  - `sqlite3` for logging interactions  \n",
    "  - `datetime` to track timestamps  \n",
    "\n",
    "- ğŸ—„ï¸ **Sets up a SQLite database (`chat_log.db`)** â€“  \n",
    "  - Creates a **`chat_history`** table (if it doesnâ€™t exist)  \n",
    "  - Stores **`prompt`**, **`response`**, and **timestamp** for each chat  \n",
    "\n",
    "- ğŸ’¬ **Defines `chat_with_model(prompt)`** â€“  \n",
    "  - Sends user input to **Ollama (Gemma 2B)**  \n",
    "  - Logs the chat to **SQLite**  \n",
    "  - Returns the modelâ€™s response  \n",
    "\n",
    "- ğŸ¨ **Creates a Gradio UI (`iface`)** â€“  \n",
    "  - **ğŸ“ Input:** A text box for user queries  \n",
    "  - **ğŸ–¥ï¸ Output:** The modelâ€™s response  \n",
    "  - **ğŸ“œ Description:** Informs users that chats are logged  \n",
    "\n",
    "- ğŸš€ **Launches the app!** â€“ Runs a browser-based chatbot with full logging  \n",
    "\n",
    "This setup ensures we can **track every interaction**, making debugging, evaluation, and iteration much easier. Next, let's test it out! ğŸ”  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ğŸ” Exploring Your Data with Datasette  \n",
    "\n",
    "![Alt text](img/datasette.png)\n",
    "\n",
    "Once weâ€™ve logged conversations in **SQLite**, we need an easy way to inspect and analyze them.  \n",
    "Thatâ€™s where **[Datasette](https://datasette.io/)** comes inâ€”a powerful tool for **browsing, querying, and exporting SQLite databases** effortlessly.  \n",
    "\n",
    "### ğŸš€ Why Datasette?  \n",
    "- **Instant Database UI** â€“ No SQL knowledge required; just open a browser and explore!  \n",
    "- **Lightning Fast** â€“ Designed for large-scale data publishing but perfect for small logs too.  \n",
    "- **Built-in Querying** â€“ Filter, sort, and search directly in a web-based UI.  \n",
    "- **Easy Exporting** â€“ Convert your database into **CSV**, **JSON**, or other formats with a click.  \n",
    "\n",
    "### ğŸ“¤ Exporting Traces to CSV  \n",
    "\n",
    "Weâ€™ll use **Datasette** to **export chat logs to a CSV file**, making it easier to analyze failure cases and refine our AI system.  \n",
    "This CSV can be used for:  \n",
    "- **ğŸ“Š Failure Mode Analysis** â€“ Identify common mistakes by reviewing responses.  \n",
    "- **ğŸ‘¥ Sharing with Subject Matter Experts** â€“ Non-technical teammates can review and give feedback.  \n",
    "- **âœ… Manual Evaluation** â€“ Open in a spreadsheet and score outputs with ğŸ‘/ğŸ‘ + comments.  \n",
    "- **ğŸ“ˆ Starting Systematic Evaluations** â€“ Lay the groundwork for automated performance tracking.  \n",
    "\n",
    "---\n",
    "\n",
    "Next, letâ€™s load up **Datasette**, explore our logged chats, and **export them for further analysis!** ğŸ§ğŸ“Š  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Recap: What We Learned  \n",
    "\n",
    "In this notebook, we focused on building the **first version of an LLM-powered classification system** that runs **entirely locally**. Hereâ€™s what we covered:  \n",
    "\n",
    "### ğŸ—ï¸ **Building an MVP AI System**  \n",
    "- Used **Gemma 2B** + **Ollama** to create a **zero-shot classification model**.  \n",
    "- Built an interactive **Gradio** UI to test our system.  \n",
    "\n",
    "### ğŸ” **Logging & Observability**  \n",
    "- Stored model interactions in an **SQLite** database for **tracing and debugging**.  \n",
    "- Used **Datasette** to **browse logged conversations and export data**.  \n",
    "\n",
    "### ğŸ“¤ **Exporting for Further Exploration**  \n",
    "- Learned how to **export chat logs to CSV** for potential later analysis.  \n",
    "- Discussed how **structured logs** help track model responses over time.  \n",
    "\n",
    "### ğŸš€ **Why This Matters**  \n",
    "- **AI systems are more than just modelsâ€”they need observability and traceability.**  \n",
    "- **Logging interactions** makes debugging, iteration, and improvement possible.  \n",
    "- This lays the **foundation** for deeper **evaluation techniques** in upcoming sections.  \n",
    "\n",
    "In the next notebook, weâ€™ll take things further by **evaluating our model using vibes**â€”testing different prompts and seeing how well they work. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
