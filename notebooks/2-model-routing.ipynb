{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Creating a Basic Image Agent\n",
    "\n",
    "![Alt text](img/augLLMs.png)\n",
    "\n",
    "In this tutorial we'll be making a simplified image classifier/agent with Gemma3.\n",
    "\n",
    "Theres two parts\n",
    "\n",
    "* **Multimodal Gemma Classifier** - Using a Gemma model to detect what's in the image and provide a specific output.\n",
    "* **Downstream Action** - A simple function that can process the results of the action, such as sending an email or anything else!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now that we have an image model ready, let's set up a simple function to interact with the model and its outputs. sLet's redefine `model_call(prompt)` function that sends user input to the LLM and receives a response.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This image shows a **dachshund dog dressed in a hot dog costume**. \n",
       "\n",
       "The costume is designed to look like a full hot dog with a bun, ketchup, and mustard on top of the dog's back. It's a playful and humorous outfit!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import pprint\n",
    "from IPython.display import Markdown\n",
    "\n",
    "image_path = \"img/NotHotDog.jpg\"  # Replace with the actual path to your image file\n",
    "\n",
    "\n",
    "def image_chat(prompt, img_path, model=\"gemma3:27b-it-qat\"):\n",
    "    response = chat(\n",
    "        model = model, \n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "                'images': [img_path]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "prompt = \"What is this an image of?\"\n",
    "output = image_chat(prompt, image_path)\n",
    "\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot Dog or not Hotdog Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"img/NotHotDog.jpg\"  # Replace with the actual path to your image file\n",
    "\n",
    "def image_chat(prompt, img_path, model=\"gemma3:27b-it-qat\"):\n",
    "    response = chat(\n",
    "        model = model, \n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "                'images': [img_path]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "prompt = 'Is this an image of the food item hot dog say yes, otherwise say no, no other output'\n",
    "image_chat(prompt, image_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"img/NotHotDog.jpg\"  # Replace with the actual path to your image file\n",
    "\n",
    "def image_chat(prompt, img_path):\n",
    "    response = chat(\n",
    "        model=\"gemma3:27b-it-qat\",  # Use a vision-capable model like LLaVA\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'Is this an image of the food item hot dog say yes, otherwise say no, no other output',\n",
    "                'images': [img_path]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "image_chat(None,image_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the response\n",
    "With our prompt complete we can turn this into a simple classifier. From here you can replace this with any python logic you like, whether its sending an email, or anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"img/Hot_dog_with_mustard.png\"  # Replace with the actual path to your image file\n",
    "\n",
    "image_chat(None, image_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give treat'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def call_a_tool(img_path):\n",
    "    \"\"\"This just prints a string, but it could be anything else\"\"\"\n",
    "    response = image_chat(None, image_path)\n",
    "    if response.lower() == 'yes':\n",
    "        return \"Give treat\"\n",
    "    return \"Add ketchup\"\n",
    "\n",
    "call_a_tool(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upgrading with Gemini \n",
    "If you need a more powerful model it's only one API call away. With this the model stops being local but in turn you get access to frontier capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a dachshund dog wearing a hot dog costume."
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "with open(image_path, 'rb') as f:\n",
    "      image_bytes = f.read()\n",
    "\n",
    "def generate():\n",
    "    client = genai.Client(\n",
    "        api_key=\"INSERT_API_KEY\",\n",
    "    )\n",
    "\n",
    "    # Pick any model from AI studio\n",
    "    model = \"gemini-2.0-flash\"\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                    types.Part.from_bytes(\n",
    "                    data=image_bytes,\n",
    "                    mime_type='image/jpeg',\n",
    "                  ),\n",
    "                types.Part.from_text(text=\"\"\"What is this?\"\"\"),\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/plain\",\n",
    "    )\n",
    "\n",
    "    for chunk in client.models.generate_content_stream(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config,\n",
    "    ):\n",
    "        print(chunk.text, end=\"\")\n",
    "generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Recap: What We Learned \n",
    "\n",
    "In this section, we built our first basic image classification agent that can differentiate between two images and respond accordingly.\n",
    "\n",
    "Here are the key ideas to remember:\n",
    "- **Not everything needs to be chat**: Models can be prompted to return parseable outputs quite easily, no architecture changes needed.\n",
    "- **The model can be an intermediate part of a system**: The model doesn't always need to be front and center of every application.\n",
    "- **From there you can do anything**: We just outputted strings, but with python (or any other language) we can make our system act agentically do anything.\n",
    "\n",
    "This pattern â€” LLM suggests, framework acts â€” is the foundation for building more complex agents later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
