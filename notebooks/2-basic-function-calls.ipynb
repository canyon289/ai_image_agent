{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Creating a Basic Image Agent\n",
    "\n",
    "![Alt text](img/augLLMs.png)\n",
    "\n",
    "In this tutorial we'll be making a simplified image classifier/agent with Gemma3.\n",
    "\n",
    "Theres two parts\n",
    "\n",
    "* **Multimodal Gemma Classifier** - Using a Gemma model to \n",
    "\n",
    "* **Downstream Action** - A simple function that can process the results of the action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a Basic Prompt to the Model\n",
    "\n",
    "Now that we have a tool ready, let's set up a simple function to interact with the model.\n",
    "\n",
    "We'll define a basic `model_call(prompt)` function that sends user input to the LLM and receives a response.  \n",
    "This will be the foundation we build on as we add more complex behaviors like tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello everyone! ðŸ˜Š \\n\\nItâ€™s great to be here with you all today. \\n\\nHowâ€™s everyone doing?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "# 12b is better so try and use it first\n",
    "model = 'gemma3:4b'\n",
    "\n",
    "\n",
    "# Note, the argument model_prompt is specific here\n",
    "def model_call(model_prompt):\n",
    "    \n",
    "    response: ChatResponse = chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': model_prompt,\n",
    "      },\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "user_prompt = \"Say hello to the class\"\n",
    "\n",
    "# Note, the argument user_prompt is specific here\n",
    "model_call(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding an Image\n",
    "Gemma3 has been trained with multimodality, where images are converted into embedding vectors the model can operate on. As a user you adding an image is quite straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dachshund (a type of dog, often called a \"wiener dog\") wearing a hot dog costume! Itâ€™s a playful and funny outfit, capitalizing on the dogâ€™s long shape to resemble a hot dog bun with a sausage and mustard on top.  It\\'s a popular costume for Halloween or just for fun!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"img/NotHotDog.jpg\"  # Replace with the actual path to your image file\n",
    "\n",
    "response = chat(\n",
    "        model=\"gemma3:27b-it-qat\",  # Use a vision-capable model like LLaVA\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'What is this?',\n",
    "                'images': [image_path]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning our image checker into a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"img/NotHotDog.jpg\"  # Replace with the actual path to your image file\n",
    "\n",
    "def image_chat(prompt, img_path):\n",
    "    response = chat(\n",
    "        model=\"gemma3:27b-it-qat\",  # Use a vision-capable model like LLaVA\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'Is this an image of the food item hot dog say yes, otherwise say no, no other output',\n",
    "                'images': [img_path]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "image_chat(None,image_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the response\n",
    "With our prompt complete we can turn this into a simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"img/Hot_dog_with_mustard.png\"  # Replace with the actual path to your image file\n",
    "\n",
    "image_chat(None, image_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give treat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_response(img_path):\n",
    "    response = image_chat(None, image_path)\n",
    "    if response.lower() == 'yes':\n",
    "        return \"Give treat\"\n",
    "    return \"Add ketchup\"\n",
    "\n",
    "parse_response(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Recap: What We Learned (THIS will be updated)\n",
    "\n",
    "In this section, we built our first basic agent that can recognize when a tool call is needed and respond accordingly.\n",
    "\n",
    "Here are the key ideas to remember:\n",
    "- **Function calls aren't magic**: We rely on the model's \"reasoning\" to decide when to call a function, based on the system prompt and user input.\n",
    "- **The model doesn't actually call functions itself**: It simply outputs a structured signal (like JSON) suggesting what should happen.  \n",
    "- **The framework â€” your code â€” decides what to do**: It parses the modelâ€™s output, calls tools when needed, and can reinject the results for a better final answer.\n",
    "\n",
    "This pattern â€” LLM suggests, framework acts â€” is the foundation for building more complex agents later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
