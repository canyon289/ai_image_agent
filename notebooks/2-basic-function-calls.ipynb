{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Creating a Basic Agent\n",
    "\n",
    "![Alt text](img/augLLMs.png)\n",
    "\n",
    "Let's walk through the fundamentals of building a basic agent.\n",
    "\n",
    "At a high level, agents aren't magical â€” they are just LLMs augmented with the ability to call external tools when needed.  \n",
    "In fact, a very simple agent is often just **an LLM in a while loop**, deciding when to call tools based on what the user asks.\n",
    "\n",
    "The key concepts we'll cover:\n",
    "- **Function Definition**: What tools the LLM can access.\n",
    "- **Function Call**: A special output from the model indicating that a tool should be used.\n",
    "- **Function Response**: The result we get from executing that tool.\n",
    "\n",
    "To tie all of this together, we'll build a minimal **LLM Framework** that wraps around the model's outputs.\n",
    "\n",
    "We'll also relate this to a broader view:  \n",
    "In more complex systems, agents often combine **retrieval**, **tools**, and **memory** â€” but at the core, the pattern remains the same:  \n",
    "The LLM outputs a call, and the framework decides what to do next."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![functioncalling](img/function-calling-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First LLM Tool\n",
    "\n",
    "Before we build our agent, we need at least one tool that the LLM can call.\n",
    "\n",
    "This is a simple (and arbitrary) tool â€” but the same pattern applies to any function you might want the model to use.\n",
    "\n",
    "We'll start by building a very basic weather lookup function, because it's intuitive and lets us focus on the function call mechanics rather than the tool itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import json\n",
    "\n",
    "def get_weather(city):\n",
    "    logging.info(\"Called weather function %s\", city)\n",
    "    if city.lower() in {\"Austin\", \"Sydney\"}:\n",
    "        return \"sunny\"\n",
    "    else:\n",
    "        return \"cloudy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cloudy'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weather(\"Austin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a Basic Prompt to the Model\n",
    "\n",
    "Now that we have a tool ready, let's set up a simple function to interact with the model.\n",
    "\n",
    "We'll define a basic `model_call(prompt)` function that sends user input to the LLM and receives a response.  \n",
    "This will be the foundation we build on as we add more complex behaviors like tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello everyone! ðŸ˜Š \\n\\nIt's great to be here. ðŸ˜Š \\n\\nHow's everyone doing today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "# 12b is better so try and use it first\n",
    "#model = 'gemma3:12b-it-qat'\n",
    "model = 'gemma3:4b-it-qat'\n",
    "\n",
    "\n",
    "# Note, the argument model_prompt is specific here\n",
    "def model_call(model_prompt):\n",
    "    \n",
    "    response: ChatResponse = chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': model_prompt,\n",
    "      },\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "user_prompt = \"Say hello to the class\"\n",
    "\n",
    "# Note, the argument user_prompt is specific here\n",
    "model_call(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guiding the Model with a System Prompt\n",
    "\n",
    "System prompts let us control how the model behaves before it sees the user's input.  \n",
    "We'll define a simple function that prepends a system instruction to the user's message, allowing us to guide tone, style, or available tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_model_call(system_prompt, user_prompt, print_prompt = False):\n",
    "    combined_prompt = f\"{system_prompt}\\n{user_prompt}\"\n",
    "\n",
    "    if print_prompt:\n",
    "        print(combined_prompt)\n",
    "    \n",
    "    return model_call(combined_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talk like a pirate to everyone. \n",
      " \n",
      "Say hello to the class\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Ahoy there, mateys! \\n\\nShiver me timbers, a hearty 'How do ye be?' to ye all! Let's be havin' a grand time learnin' and explorin' together!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example system prompt: talk like a pirate\n",
    "# This is injected by the LLM application behind the scenes\n",
    "\n",
    "system_prompt = '''Talk like a pirate to everyone. \\n '''\n",
    "augmented_model_call(system_prompt, user_prompt, print_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teaching the Model to Suggest Tool Calls\n",
    "\n",
    "Now that we've seen how system prompts can guide behavior, we'll write a system prompt that teaches the model how to suggest when a tool should be used.  \n",
    "We'll structure the prompt so that the model outputs a JSON instruction when it decides a function call is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You have the following functions available\n",
    " def get_weather(city: str)\n",
    "   \"\"\"Given a city returns the weather for that city\"\"\n",
    "\n",
    " If you call this function return the json [{\"city\": city}] and nothing else\n",
    " otherwise respond normally\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[{\"city\": \"Sydney\"}]\\n```'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"What's the weather in Sydney?\"\n",
    "augmented_model_call(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[{\"city\": \"Austin\"}]\\n```\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"How's the Austin forecast looking today?\"\n",
    "augmented_model_call(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[{\"city\": \"London\"}]\\n```'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"How's the London weather looking today?\"\n",
    "augmented_model_call(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Response\n",
    "\n",
    "Now that the model knows how to suggest tool calls, we need to build a small framework around it to actually process results.\n",
    "\n",
    "The basic logic we want:\n",
    "\n",
    "1. If there is **no function call**, simply return the model's response to the user.\n",
    "2. If there **is a function call**:\n",
    "   - a. Call the appropriate tool to get new information.\n",
    "   - b. Either return the tool result directly, or reinject it back into the model to generate a better final response.\n",
    "\n",
    "We'll start by writing a simple function to detect when the model outputs a tool call in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 'Austin'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = f\"```json\\n(.*?)\\n``\"\n",
    "\n",
    "def parse_response(model_response):\n",
    "    if tool_call := re.search(pattern, model_response):\n",
    "        return json.loads(tool_call.groups(0)[0])[0]\n",
    "\n",
    "model_response = '```json\\n[{\"city\": \"Austin\"}]\\n```\\n'\n",
    "parse_response(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together: Building Our First Agent\n",
    "\n",
    "Now that we've built all the individual pieces, let's combine them to create our first real agent.  \n",
    "This agent will:\n",
    "- Send a prompt to the model.\n",
    "- Detect if a tool call is needed.\n",
    "- Call the tool and get the result.\n",
    "- Optionally feed the tool result back into the model for a better final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! ðŸ˜Š\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_interaction(user_prompt):\n",
    "    system_prompt = '''\n",
    "    You have the following functions available:\n",
    "\n",
    "    def get_weather(city: str)\n",
    "    \"\"\"Given a city returns the weather for that city\"\"\"\n",
    "\n",
    "    If you call this function return the json [{\"city\": city}] and nothing else\n",
    "    otherwise respond normally\n",
    "    '''\n",
    "\n",
    "    # Get a model response. Right now we don't know if it's a function call or chat response\n",
    "    model_response = augmented_model_call(system_prompt, user_prompt)\n",
    "\n",
    "    # Regex to see if we have the json which indicates a function call\n",
    "    function_call_json = parse_response(model_response)\n",
    "\n",
    "    # If it's not a function call, return the response\n",
    "    if not function_call_json:\n",
    "        return model_response\n",
    "    \n",
    "    # Since we detect a function call\n",
    "    weather = get_weather(function_call_json[\"city\"])\n",
    "\n",
    "    # We have a choice here\n",
    "    # We could return the weather directly to the user\n",
    "    # But for a nicer experience let's reinject it into the LLM for a better final response\n",
    "    function_response_prompt = f\"The weather in {function_call_json['city']} is {weather}, tell me the weather nicely\"\n",
    "\n",
    "    # We already checked for weather so we don't need to go again\n",
    "    model_response = model_call(function_response_prompt)\n",
    "    return model_response\n",
    "\n",
    "chat_interaction(\"Can you say hi to class?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[\\n  {\\n    \"city\": \"London\"\\n  }\\n]\\n```\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_interaction(\"What's the weather today in London?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```tool_code\\nget_weather(city=\"Austin\")\\n```'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_interaction(\"How are things looking in Austin?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Recap: What We Learned\n",
    "\n",
    "In this section, we built our first basic agent that can recognize when a tool call is needed and respond accordingly.\n",
    "\n",
    "Here are the key ideas to remember:\n",
    "- **Function calls aren't magic**: We rely on the model's \"reasoning\" to decide when to call a function, based on the system prompt and user input.\n",
    "- **The model doesn't actually call functions itself**: It simply outputs a structured signal (like JSON) suggesting what should happen.  \n",
    "- **The framework â€” your code â€” decides what to do**: It parses the modelâ€™s output, calls tools when needed, and can reinject the results for a better final answer.\n",
    "\n",
    "This pattern â€” LLM suggests, framework acts â€” is the foundation for building more complex agents later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn: Connect to a Real API\n",
    "\n",
    "Now that you've seen how we can define simple tools manually,  \n",
    "let's try using a real external API to fetch live data.\n",
    "\n",
    "Below is a starting point â€” your task is to plug this into an agent loop, just like we did earlier with our fake `get_weather` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(latitude, longitude):\n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it by fetching the current temperature for London!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LONDON_LATITUDE = 51.5074 # Approximate latitude for London\n",
    "LONDON_LONGITUDE = -0.1278 # Approximate longitude for London (West is negative)\n",
    "get_weather(LONDON_LATITUDE, LONDON_LONGITUDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Call versus Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![functioncalling](img/AlwaysHasBeen.jpg)\n",
    "Source: https://huggingface.co/blog/tiny-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Agents have a lot of definitions.  \n",
    "The simplest is that they're a model that can call tools inside a while loop.\n",
    "\n",
    "The model suggests an action, but it's our code that decides what actually happens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
